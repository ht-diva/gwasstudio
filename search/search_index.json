{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to GWASStudio documentation","text":"<p>GWASStudio is a powerful CLI tool designed for efficient storage, retrieval, and querying of genomic summary statistics. It offers a high-performance infrastructure for handling and analyzing large-scale GWAS and QTL datasets, enabling seamless cross-dataset exploration.</p>"},{"location":"#commands","title":"Commands","text":"<ul> <li><code>export</code> - Export summary statistics from the DB with various filtering options.</li> <li><code>info</code> - Show GWASStudio details</li> <li><code>ingest</code> - Ingest datasets into the DB.</li> <li><code>list</code> - List every category \u2192 project \u2192 study hierarchy stored in the metadata DB.</li> <li><code>meta-query</code> - Query the DB for specific records.</li> </ul>"},{"location":"#installation","title":"Installation","text":"<p>Set up the GWASStudio CLI on your system.</p>"},{"location":"#getting-started","title":"Getting Started","text":"<p>This short overview of GWASStudio will help you get started.</p>"},{"location":"#examples","title":"Examples","text":"<p>Showcase GWASStudio's common workflows by using examples.</p>"},{"location":"#projects","title":"Projects","text":"<p>Overview of GWASStudio projects.</p>"},{"location":"changelog/","title":"Changelog","text":"<p>All notable changes to this project will be documented in this file.</p> <p>Git history</p>"},{"location":"changelog/#2100-2025-11-03","title":"[2.10.0] - 2025-11-03","text":""},{"location":"changelog/#features","title":"\ud83d\ude80 Features","text":"<ul> <li>Add locus-flanks option</li> <li>Combine regions and snps filtering</li> <li>Add metadata broadcasting and skip option</li> </ul>"},{"location":"changelog/#bug-fixes","title":"\ud83d\udc1b Bug Fixes","text":"<ul> <li>Increase TileDB timeout, close #102</li> <li>Handle empty dataframe</li> <li>Restore file format</li> </ul>"},{"location":"changelog/#refactor","title":"\ud83d\ude9c Refactor","text":"<ul> <li>Dataframe output for extraction methods</li> </ul>"},{"location":"changelog/#styling","title":"\ud83c\udfa8 Styling","text":"<ul> <li>Linting</li> </ul>"},{"location":"changelog/#290-2025-10-22","title":"[2.9.0] - 2025-10-22","text":""},{"location":"changelog/#features_1","title":"\ud83d\ude80 Features","text":"<ul> <li>Add p-value filtering</li> </ul>"},{"location":"changelog/#bug-fixes_1","title":"\ud83d\udc1b Bug Fixes","text":"<ul> <li>Return empty df with non-existent region/snps, close #98</li> <li>Always return two df in _locus_breaker</li> <li>Wait indefinitely for slurm workers</li> <li>Move maf and phenovar to locusbreaker options</li> <li>Add group_name to the output filename to avoid overwriting</li> </ul>"},{"location":"changelog/#other","title":"\ud83d\udcbc Other","text":"<ul> <li>Bump version</li> </ul>"},{"location":"changelog/#284-2025-10-17","title":"[2.8.4] - 2025-10-17","text":""},{"location":"changelog/#bug-fixes_2","title":"\ud83d\udc1b Bug Fixes","text":"<ul> <li>Add wait time for Dask workers, close #90</li> <li>SNPs filtering output with header and overwrite, close #89</li> <li>Set the dedup_coords configuration variable to its default value</li> </ul>"},{"location":"changelog/#other_1","title":"\ud83d\udcbc Other","text":"<ul> <li>Bump version</li> </ul>"},{"location":"changelog/#miscellaneous-tasks","title":"\u2699\ufe0f Miscellaneous Tasks","text":"<ul> <li>Ensure that the version is filtered correctly</li> <li>Add a Makefile target for version bumping and changelog generation, close #99</li> <li>Set the current version</li> <li>Add git-cliff ignore file</li> <li>Fix bump-version target</li> </ul>"},{"location":"changelog/#283-2025-10-13","title":"[2.8.3] - 2025-10-13","text":""},{"location":"changelog/#bug-fixes_3","title":"\ud83d\udc1b Bug Fixes","text":"<ul> <li>Adjiust site_url</li> <li>Add a field to the output_fields list used in the load_search_topics function, close #88</li> <li>Add the missing field to the metadata_utils unit test, close #94</li> </ul>"},{"location":"changelog/#other_2","title":"\ud83d\udcbc Other","text":"<ul> <li>Change auth to simple</li> <li>Add image as option</li> <li>Make docker image</li> <li>Format mem</li> <li>Remove time &amp; close gateway</li> <li>Update cluster connection &amp; wait</li> <li>Add error handling for client connection</li> <li>Debug connection loss</li> <li>Explicitly pass client</li> <li>Writable log_dir</li> <li>Writable all dirs</li> <li>Writable all dirs debug</li> <li>Writable all dirs update</li> <li>Prepare PR</li> <li>Bump version</li> </ul>"},{"location":"changelog/#refactor_1","title":"\ud83d\ude9c Refactor","text":"<ul> <li>Do some linting</li> </ul>"},{"location":"changelog/#documentation","title":"\ud83d\udcda Documentation","text":"<ul> <li>Update README</li> <li>Add main page and details about installation</li> <li>Add getting started info</li> <li>Typo</li> <li>Unused</li> <li>Fix url</li> <li>Minor</li> <li>Minor updates for export</li> <li>Add the following tables to the docs:</li> <li>Add projects page &amp; ukb/-ppp info</li> <li>Add decode, finngen, g&amp;h info</li> <li>Improve the introduction</li> <li>Add the examples section</li> <li>Add a changelog to the documentation</li> </ul>"},{"location":"changelog/#styling_1","title":"\ud83c\udfa8 Styling","text":"<ul> <li>Update changelog template</li> <li>Linting</li> </ul>"},{"location":"changelog/#miscellaneous-tasks_1","title":"\u2699\ufe0f Miscellaneous Tasks","text":"<ul> <li>Update pre-commit tools</li> <li>Fix typos package name</li> <li>Add output</li> <li>Commit the changelog to the repo</li> <li>Update generate changelog action</li> <li>Comment out the changelog commit, as it isn't working at the moment</li> </ul>"},{"location":"changelog/#282-2025-09-25","title":"[2.8.2] - 2025-09-25","text":""},{"location":"changelog/#features_2","title":"\ud83d\ude80 Features","text":"<ul> <li>Set a default for the uri</li> </ul>"},{"location":"changelog/#other_3","title":"\ud83d\udcbc Other","text":"<ul> <li>Bump version</li> </ul>"},{"location":"changelog/#281-2025-09-24","title":"[2.8.1] - 2025-09-24","text":""},{"location":"changelog/#features_3","title":"\ud83d\ude80 Features","text":"<ul> <li>Adjust the paths in the dockerfile so that it behaves like the conda setup, add several Dask options for SLURMCluster.</li> </ul>"},{"location":"changelog/#other_4","title":"\ud83d\udcbc Other","text":"<ul> <li>Bump version</li> </ul>"},{"location":"changelog/#280-2025-09-22","title":"[2.8.0] - 2025-09-22","text":""},{"location":"changelog/#features_4","title":"\ud83d\ude80 Features","text":"<ul> <li>Simplify Dask configuration</li> </ul>"},{"location":"changelog/#other_5","title":"\ud83d\udcbc Other","text":"<ul> <li>Bump version</li> </ul>"},{"location":"changelog/#270-2025-09-18","title":"[2.7.0] - 2025-09-18","text":""},{"location":"changelog/#bug-fixes_4","title":"\ud83d\udc1b Bug Fixes","text":"<ul> <li>Avoid sending live TileDB objects across the network; add utility for joining filesystem paths and S3 URIs</li> <li>Better logger message about the batches</li> </ul>"},{"location":"changelog/#other_6","title":"\ud83d\udcbc Other","text":"<ul> <li>Bump version</li> </ul>"},{"location":"changelog/#testing","title":"\ud83e\uddea Testing","text":"<ul> <li>Add unit tests for the list CLI module helpers</li> </ul>"},{"location":"changelog/#260-2025-09-16","title":"[2.6.0] - 2025-09-16","text":""},{"location":"changelog/#features_5","title":"\ud83d\ude80 Features","text":"<ul> <li>Add the list projects command and expose cli commands through init.py</li> </ul>"},{"location":"changelog/#bug-fixes_5","title":"\ud83d\udc1b Bug Fixes","text":"<ul> <li>Poetry-core 2.2.0 from conda-forge is broken</li> </ul>"},{"location":"changelog/#other_7","title":"\ud83d\udcbc Other","text":"<ul> <li>Bump version</li> </ul>"},{"location":"changelog/#250-2025-08-11","title":"[2.5.0] - 2025-08-11","text":""},{"location":"changelog/#features_6","title":"\ud83d\ude80 Features","text":"<ul> <li>TileDB datasets grouping</li> </ul>"},{"location":"changelog/#bug-fixes_6","title":"\ud83d\udc1b Bug Fixes","text":"<ul> <li>F-string syntax error</li> </ul>"},{"location":"changelog/#other_8","title":"\ud83d\udcbc Other","text":"<ul> <li>Bump version</li> </ul>"},{"location":"changelog/#performance","title":"\u26a1 Performance","text":"<ul> <li>Add new metadata enums and set pyarrow as backend</li> </ul>"},{"location":"changelog/#miscellaneous-tasks_2","title":"\u2699\ufe0f Miscellaneous Tasks","text":"<ul> <li>Add pandas-stubs dependency</li> <li>Remove unused stuff</li> </ul>"},{"location":"changelog/#240-2025-08-11","title":"[2.4.0] - 2025-08-11","text":""},{"location":"changelog/#features_7","title":"\ud83d\ude80 Features","text":"<ul> <li>Update Mongo query composition</li> <li>Add output prefix dictionary generator</li> </ul>"},{"location":"changelog/#bug-fixes_7","title":"\ud83d\udc1b Bug Fixes","text":"<ul> <li>Catch yaml loading error</li> </ul>"},{"location":"changelog/#other_9","title":"\ud83d\udcbc Other","text":"<ul> <li>Bump version</li> <li>Copying from lzv</li> </ul>"},{"location":"changelog/#refactor_2","title":"\ud83d\ude9c Refactor","text":"<ul> <li>Add missing column check and simplify dataframe processing</li> </ul>"},{"location":"changelog/#miscellaneous-tasks_3","title":"\u2699\ufe0f Miscellaneous Tasks","text":"<ul> <li>Add example files module</li> </ul>"},{"location":"changelog/#231-2025-07-17","title":"[2.3.1] - 2025-07-17","text":""},{"location":"changelog/#miscellaneous-tasks_4","title":"\u2699\ufe0f Miscellaneous Tasks","text":"<ul> <li>Bump version</li> </ul>"},{"location":"changelog/#230-2025-07-15","title":"[2.3.0] - 2025-07-15","text":""},{"location":"changelog/#features_8","title":"\ud83d\ude80 Features","text":"<ul> <li>Update data model, queries and ingestion.</li> <li>(extraction_methods.py) Refactor tiledb_array_query to handle errors by adjusting attributes.</li> <li>(tdb_schema) Implement TileDB schema creator</li> </ul>"},{"location":"changelog/#bug-fixes_8","title":"\ud83d\udc1b Bug Fixes","text":"<ul> <li>Replace separator in the output fields to match column data types</li> </ul>"},{"location":"changelog/#other_10","title":"\ud83d\udcbc Other","text":"<ul> <li>Update dataframe column access to use <code>.loc</code></li> <li>Make it compatible with py3.11</li> </ul>"},{"location":"changelog/#refactor_3","title":"\ud83d\ude9c Refactor","text":"<ul> <li>Update metadata query function for better performance</li> <li>Rename notes.source_id to notes_source_id in export</li> <li>Refactor TileDBSchemaCreator to use BaseEnum</li> <li> <p>refactor: Update test scripts for improved testing coverage</p> <ul> <li>Adding tests for data ingestion without pvalue</li> <li>Ensuring correct query results when using the 'search_example_01.yml' file and 'search_example_04.yml' file</li> <li>Verifying export functionality with different file formats and settings</li> </ul> </li> </ul>"},{"location":"changelog/#221-2025-07-04","title":"[2.2.1] - 2025-07-04","text":""},{"location":"changelog/#other_11","title":"\ud83d\udcbc Other","text":"<ul> <li>Bump version</li> </ul>"},{"location":"changelog/#refactor_4","title":"\ud83d\ude9c Refactor","text":"<ul> <li>Update Dask cluster deployment options for SLURM and gateway setups</li> </ul>"},{"location":"changelog/#220-2025-07-03","title":"[2.2.0] - 2025-07-03","text":""},{"location":"changelog/#features_9","title":"\ud83d\ude80 Features","text":"<ul> <li>(cli/export) Enhance export command with several improvements</li> <li>feat(cli/ingest.py): Implement TileDB SM configuration support in ingest</li> </ul> <p>This commit adds support for using the TileDB SM configuration when ingesting datasets from the command line interface (CLI). The change includes retrieving the SM configuration from the configuration file and passing it to the <code>process_and_ingest</code> function. Additionally, minor adjustments were made to import statements to reflect this new functionality.</p>"},{"location":"changelog/#bug-fixes_9","title":"\ud83d\udc1b Bug Fixes","text":"<ul> <li>fix(cli/export.py): Update attribute export options in export.py</li> </ul> <p>Updated the default attribute export options for the '--attr' argument in <code>src/gwasstudio/cli/export.py</code>. The default attribute string now includes \"MLOG10P\" as well as the original \"BETA,SE,EAF\".</p> <p>GWASStudio now ingests and exports the MLOG10P attribute using default options.</p>"},{"location":"changelog/#testing_1","title":"\ud83e\uddea Testing","text":"<ul> <li> <p>test: Add End-to-End Test Script for CLI</p> <p>This commit introduces an end-to-end test script (cli_test.sh) to the existing suite of tests. The script runs through the complete pipeline, including ingesting data, querying data, filtering by traits and regions, exporting data in different formats, and performing Locusbreaker analysis. This is aimed at ensuring the correctness and consistency of the command-line interface (CLI) functionality.</p> </li> </ul>"},{"location":"changelog/#miscellaneous-tasks_5","title":"\u2699\ufe0f Miscellaneous Tasks","text":"<ul> <li>Remove a print statement</li> <li>Bump version</li> </ul>"},{"location":"changelog/#210-2025-06-26","title":"[2.1.0] - 2025-06-26","text":""},{"location":"changelog/#features_10","title":"\ud83d\ude80 Features","text":"<ul> <li>feat(hashing): Refactor compute_hash function to improve flexibility</li> </ul> <p>If a file path is provided, the function computes the hash based on both filename and file content.    Previous behavior: Function only accepted a file path as input and returned the SHA-256 hash based on that file's content.</p> <p>This refactoring fixes #64</p>"},{"location":"changelog/#bug-fixes_10","title":"\ud83d\udc1b Bug Fixes","text":"<ul> <li>fix: Fix get_snp_list call for None case    The function now handles the case when snp_list_file is None by not calling get_snp_list, preventing a potential error.</li> </ul>"},{"location":"changelog/#refactor_5","title":"\ud83d\ude9c Refactor","text":"<ul> <li>(mongo) Update logger message for non-unique ID (warning instead of error)</li> <li>Adapt Dask SLURM cluster for auto-scaling and improved performance</li> </ul>"},{"location":"changelog/#miscellaneous-tasks_6","title":"\u2699\ufe0f Miscellaneous Tasks","text":"<ul> <li>Bump version</li> </ul>"},{"location":"changelog/#200-2025-06-23","title":"[2.0.0] - 2025-06-23","text":""},{"location":"changelog/#features_11","title":"\ud83d\ude80 Features","text":"<ul> <li>Remove PyArrow dependency in _process_regions and refactor related functions</li> <li>Embedded MongoDBManager with customizable port and timeout</li> <li>Add Gwasstudio CLI Test Script and related files</li> <li>feat(CLI): Add output format option to export command</li> </ul> <p>This commit introduces an option for specifying the output file format when using the export command in the CLI. The available formats are parquet, csv.gz, and csv.</p> <p>Note that the 'compression' parameter has been deprecated and replaced with a boolean flag to indicate whether the file should be compressed or not. Also, the default value for the file format is now 'csv.gz'.</p> <p>Previously, the output format was hardcoded as csv, which made it inconvenient if users wanted to export their data in a different format. With this change, users have the flexibility to choose the format that best suits their needs.</p> <p>Footer: No breaking changes or issues are closed with this commit.</p>"},{"location":"changelog/#bug-fixes_11","title":"\ud83d\udc1b Bug Fixes","text":"<ul> <li>Fix missing argument</li> <li>Update mongo manager to use default port 27018</li> <li>Reenable Dask functionality in export.py</li> <li>Switch to delayed() method for reading snp list file</li> <li>fix: Fix MongoDB URI handling for embedded and localhost configurations    The script was not properly checking if the MongoDB URI provided in the configuration was localhost or not when using an embedded MongoDB server. This commit corrects the issue by adding a condition to check if the URI contains \"localhost:27018\".</li> </ul>"},{"location":"changelog/#other_12","title":"\ud83d\udcbc Other","text":"<ul> <li>Modifications to improve space</li> <li>Adapt the export to new schema</li> <li>Add option to print the snps and pvalue</li> <li>Add scipy library</li> <li>Update <code>ingest.py</code> to include <code>pvalue</code> as a flag in the CLI options and add documentation for the new flag</li> <li>Linting and fix tiledb_iterator_query_df used before assignment</li> <li>Rename the boolean argument to clearly state the action</li> <li>Change order of traits column and locusbreaker</li> <li>Add Dask to export</li> <li>Add function for SNPID and change to write_table</li> <li>Review copilot: Moving sno list function outside dask. Modifying description of function</li> <li>Changing output_file in output_prefix</li> <li>Re-arranging functions in files</li> <li>Ruff re-formatting</li> <li>Resolving warning and empty regions error</li> <li>Modifying the README</li> <li>Adding table of contents</li> <li>Adding output</li> <li>Small edits</li> <li>Resolving review comments</li> <li>Move trait_id list calculation to handle both notes and data_id cases</li> <li>Update README.md</li> <li>Bump version</li> </ul>"},{"location":"changelog/#refactor_6","title":"\ud83d\ude9c Refactor","text":"<ul> <li>Refactor export functions for output prefix handling</li> <li>(extraction_methods) Introduce TileDB dimensions constants for consistent query usage</li> </ul>"},{"location":"changelog/#documentation_1","title":"\ud83d\udcda Documentation","text":"<ul> <li>Update README</li> </ul>"},{"location":"changelog/#testing_2","title":"\ud83e\uddea Testing","text":"<ul> <li>Add test for writing CSV with GZip compression</li> </ul>"},{"location":"changelog/#miscellaneous-tasks_7","title":"\u2699\ufe0f Miscellaneous Tasks","text":"<ul> <li>(build_decode_dictionary.py) Remove obsolete script</li> <li>Update gitignore</li> <li>Simplify cli_test.sh script by introducing a run_command function</li> <li>Update README and example files</li> <li>Check if the gwasstudio command is available</li> </ul>"},{"location":"changelog/#100-2025-05-28","title":"[1.0.0] - 2025-05-28","text":""},{"location":"changelog/#features_12","title":"\ud83d\ude80 Features","text":"<ul> <li>Add Bokeh dependency and refactor Dask cluster management</li> </ul>"},{"location":"changelog/#bug-fixes_12","title":"\ud83d\udc1b Bug Fixes","text":"<ul> <li>Fixing locusbreaker</li> <li>Fix test</li> </ul>"},{"location":"changelog/#other_13","title":"\ud83d\udcbc Other","text":"<ul> <li> <p>Makefile updates:</p> </li> <li> <p>Added create-env target to create the conda environment if it doesn't exist.</p> </li> <li>Simplified the if statements for activating the conda environment.</li> <li>Removed the mongo_docker_run, mongo_docker_stop, and m1_env targets.</li> <li>Updated the TARGETS variable to include create-env.</li> <li>Added ENV_NAME and CONDA_ACTIVATE variables for better readability and maintainability.</li> <li>Remove the project_list property and its associated methods from the ConfigurationManager class.</li> <li>Linting</li> <li>Changing region function</li> <li>Changing functions of regions</li> <li>Adding sampled files for testing</li> <li>Provide an embedded mongodb setup, fix #46</li> <li>Add an option to shorten hashes</li> <li>Use a shorter version of the hash</li> <li>Reformat the hashing function and set the default algorithm and hash length in the configuration file.</li> <li>Enable dask ingestion</li> <li>Set by default local Dask deployment</li> <li>Merge the two ingest actions into one</li> <li>Add an option to choose between metadata ingestion, data ingestion, or both</li> <li>Assume file system ingestion if scheme it is not S3</li> <li>Update Makefile</li> <li>Add search and metadata support files</li> <li>Update Getting started section in the README</li> <li>Update authors and bump version</li> <li>Update docker environment</li> </ul>"},{"location":"changelog/#095-2025-04-14","title":"[0.9.5] - 2025-04-14","text":""},{"location":"changelog/#other_14","title":"\ud83d\udcbc Other","text":"<ul> <li> <p>Update metadata processing and ingestion logic</p> </li> <li> <p>Improve <code>ingest_metadata</code> to use a generator for processing rows, reducing memory usage.</p> </li> <li>Add logging for the number of documents processed during ingestion.</li> </ul>"},{"location":"changelog/#094-2025-04-10","title":"[0.9.4] - 2025-04-10","text":""},{"location":"changelog/#bug-fixes_13","title":"\ud83d\udc1b Bug Fixes","text":"<ul> <li>Fix NoneType error; refactoring</li> <li>Fix choice</li> <li>Fix the save function to update records properly</li> <li>Fix search files</li> </ul>"},{"location":"changelog/#other_15","title":"\ud83d\udcbc Other","text":"<ul> <li>Improve wording</li> <li> <p>Refactor export functionality and add file existence check</p> </li> <li> <p>Refactored the <code>export</code> command in <code>src/gwasstudio/cli/export.py</code> to use a prefix for output files instead of a single output file.</p> </li> <li>Added a <code>check_file_exists</code> function in <code>src/gwasstudio/utils/__init__.py</code> to check if a file exists and log the appropriate message.</li> <li>Updated the import statements in <code>src/gwasstudio/cli/export.py</code> to include the new <code>check_file_exists</code> function and <code>get_mongo_uri</code> function.</li> <li>Updated the <code>export</code> function to use the new <code>check_file_exists</code> function, the <code>output_prefix</code> instead of <code>output_file</code> and <code>get_mongo_uri</code> function to retrieve mongodb details.</li> <li>Bumped code version.</li> <li> <p>Refactor data export and metadata query functionality</p> </li> <li> <p>Refactored the <code>export</code> command in <code>src/gwasstudio/cli/export.py</code> to use the <code>write_table</code> function for writing DataFrames to disk.</p> </li> <li>Updated the <code>meta_query</code> command in <code>src/gwasstudio/cli/metadata/query.py</code> to use the <code>write_table</code> function for writing query results.</li> <li>Removed the deprecated <code>df_to_csv</code> function from <code>src/gwasstudio/utils/metadata.py</code>.</li> <li>Added unit tests for the <code>write_table</code> function in <code>tests/unit/test_utils.py</code>.</li> <li>Updated the <code>write_table</code> function in <code>src/gwasstudio/utils/__init__.py</code> to include a logger parameter and handle custom log messages.</li> <li>Bumped code version</li> </ul>"},{"location":"changelog/#092-2025-04-04","title":"[0.9.2] - 2025-04-04","text":""},{"location":"changelog/#other_16","title":"\ud83d\udcbc Other","text":"<ul> <li>Bump version</li> <li>Write metadata query result during export</li> <li>Update pre-commit config</li> <li>Update README</li> <li>Add Vault mount point option and update Vault integration</li> </ul>"},{"location":"changelog/#091-2025-03-19","title":"[0.9.1] - 2025-03-19","text":""},{"location":"changelog/#other_17","title":"\ud83d\udcbc Other","text":"<ul> <li>Refactor Dask deployment options, update cluster creation and client setup in dask_client.py, ingest.py and main.py</li> <li>Remove comments</li> <li>Remove other comments</li> <li>Add metadata to export</li> <li>Adding searchfile example</li> <li>Data.id does not exist in the DB, data_id will be added to the result by default</li> <li>Remove duplicated options</li> <li>Make the options consistent</li> <li>Pyarrow is needed</li> <li>Rename <code>format</code> to <code>file_format</code> to avoid shadowing the built-in Python function and other fixes</li> <li> <p>Refactor config handling and reorganize code structure</p> </li> <li> <p>Rename cfg to tiledb in context object for clarity</p> </li> <li>Create new utils/cfg.py module for centralized configuration management</li> <li>Move metadata utilities from cli/metadata/utils.py to utils/metadata.py</li> <li>Enhance Dask deployment checks with specific deployment types</li> <li>Update import statements across codebase to reflect new structure</li> </ul>"},{"location":"changelog/#refactor_7","title":"\ud83d\ude9c Refactor","text":"<ul> <li>Refactor export.py</li> </ul>"},{"location":"changelog/#090-2025-03-14","title":"[0.9.0] - 2025-03-14","text":""},{"location":"changelog/#other_18","title":"\ud83d\udcbc Other","text":"<ul> <li>Reformat metadata util functions</li> <li>Implement functions to retrieve configuration data from HashCorp vault</li> <li>Bump version</li> </ul>"},{"location":"changelog/#refactor_8","title":"\ud83d\ude9c Refactor","text":"<ul> <li>Refactor the meta-query</li> </ul>"},{"location":"changelog/#082-2025-03-12","title":"[0.8.2] - 2025-03-12","text":""},{"location":"changelog/#bug-fixes_14","title":"\ud83d\udc1b Bug Fixes","text":"<ul> <li>Fix missing dependency; add deptry package to check for issues with dependencies</li> </ul>"},{"location":"changelog/#081-2025-03-12","title":"[0.8.1] - 2025-03-12","text":""},{"location":"changelog/#other_19","title":"\ud83d\udcbc Other","text":"<ul> <li>Add better logging options</li> <li>Add Dask client option</li> <li>Some linting</li> <li>Resolve comments</li> <li>Follow the same convention for the CLI options</li> <li>Bump version</li> </ul>"},{"location":"changelog/#080-2025-03-09","title":"[0.8.0] - 2025-03-09","text":""},{"location":"changelog/#bug-fixes_15","title":"\ud83d\udc1b Bug Fixes","text":"<ul> <li>Fix arch, make docker image lighter</li> </ul>"},{"location":"changelog/#other_20","title":"\ud83d\udcbc Other","text":"<ul> <li>Build multi platform docker image</li> <li>Update conda lock files</li> <li>Slim down the docker image</li> <li>Make hadolint happy</li> <li>Update python to version 3.12</li> <li>Update test action</li> <li>Update base environment</li> <li>Update the base environment</li> <li>Fit the test action</li> </ul>"},{"location":"changelog/#070-2025-03-03","title":"[0.7.0] - 2025-03-03","text":""},{"location":"changelog/#bug-fixes_16","title":"\ud83d\udc1b Bug Fixes","text":"<ul> <li>Fix test</li> </ul>"},{"location":"changelog/#other_21","title":"\ud83d\udcbc Other","text":"<ul> <li>Add notes as field to store generic content; improve JSON support for storing JSON-formatted data within MongoDB</li> <li>Modifying LB</li> <li>Modifying locusbreaker and adding S and NEFF</li> <li>Answer review gmauro</li> <li>Linting, reformatting and fixing an import</li> <li>Make a lighter image</li> <li>Add arm64 build</li> <li>Create the s3 bucket if if it does not exists</li> <li>Add make</li> <li>Try ubuntu-arm runner</li> <li>Bump version</li> </ul>"},{"location":"changelog/#068-2025-02-25","title":"[0.6.8] - 2025-02-25","text":""},{"location":"changelog/#bug-fixes_17","title":"\ud83d\udc1b Bug Fixes","text":"<ul> <li>Fix variable name</li> </ul>"},{"location":"changelog/#067-2025-02-25","title":"[0.6.7] - 2025-02-25","text":""},{"location":"changelog/#bug-fixes_18","title":"\ud83d\udc1b Bug Fixes","text":"<ul> <li>Fix SSL check</li> </ul>"},{"location":"changelog/#066-2025-02-25","title":"[0.6.6] - 2025-02-25","text":""},{"location":"changelog/#other_22","title":"\ud83d\udcbc Other","text":"<ul> <li>Add s3 credentials</li> <li>Bump version</li> </ul>"},{"location":"changelog/#065-2025-02-24","title":"[0.6.5] - 2025-02-24","text":""},{"location":"changelog/#other_23","title":"\ud83d\udcbc Other","text":"<ul> <li>Typo</li> <li>Add type hint</li> <li>Bump version</li> </ul>"},{"location":"changelog/#064-2025-02-24","title":"[0.6.4] - 2025-02-24","text":""},{"location":"changelog/#bug-fixes_19","title":"\ud83d\udc1b Bug Fixes","text":"<ul> <li>Fix parse_uri</li> </ul>"},{"location":"changelog/#other_24","title":"\ud83d\udcbc Other","text":"<ul> <li>Bump version</li> </ul>"},{"location":"changelog/#063-2025-02-24","title":"[0.6.3] - 2025-02-24","text":""},{"location":"changelog/#other_25","title":"\ud83d\udcbc Other","text":"<ul> <li>Create tiledb schema</li> <li>Bump version</li> </ul>"},{"location":"changelog/#062-2025-02-23","title":"[0.6.2] - 2025-02-23","text":""},{"location":"changelog/#other_26","title":"\ud83d\udcbc Other","text":"<ul> <li>Enable ingestion to fs</li> </ul>"},{"location":"changelog/#061-2025-02-23","title":"[0.6.1] - 2025-02-23","text":""},{"location":"changelog/#other_27","title":"\ud83d\udcbc Other","text":"<ul> <li>Add mongodb uri as a cli option</li> <li>Meta_query use mongodb uri option</li> <li>Bump version</li> </ul>"},{"location":"changelog/#060-2025-02-19","title":"[0.6.0] - 2025-02-19","text":""},{"location":"changelog/#other_28","title":"\ud83d\udcbc Other","text":"<ul> <li>Update meta_ingest command to ingest metadata from a JSON array</li> <li>Add whole GWAS output option</li> <li>Linting</li> <li>Reformat meta_query cli file</li> <li>Add study to the unique_key</li> <li>Update metadata ingest and query to handle input from table file</li> <li>Improve meta-query logic fixing multi-key queries for trait, add case-sensitive option for the search</li> <li>Bump version</li> </ul>"},{"location":"changelog/#refactor_9","title":"\ud83d\ude9c Refactor","text":"<ul> <li>Refactor meta_query to handle trait dictionaries with more elements</li> </ul>"},{"location":"changelog/#051-2025-01-15","title":"[0.5.1] - 2025-01-15","text":""},{"location":"changelog/#bug-fixes_20","title":"\ud83d\udc1b Bug Fixes","text":"<ul> <li>Fix print</li> <li>Fix mlog10p function</li> <li>Fix ge, le filtering functions and add min</li> <li>Fix few parameters</li> <li>Fix few parameters and add option for using an external list of SNPs</li> <li>Fix function name</li> <li>Fix few parameters to make the pipeline work</li> <li>Fix for ImportError: cannot import name '_manylinux' from 'packaging'</li> <li>Fix connection manager</li> <li>Fix env</li> <li>Fix export version2</li> <li>Fix import</li> <li>Fix multiple errors, some code was only commented</li> <li>Fix imports</li> </ul>"},{"location":"changelog/#other_29","title":"\ud83d\udcbc Other","text":"<ul> <li>Add filter on pvalue</li> <li>Add mlog10p_more_than filter and fix mlog10p values</li> <li>Relax scipy version</li> <li>Add the mlog10p maximum value for each sample</li> <li>Add samples option</li> <li>Proper output format</li> <li>Add ingestion and s3 configuration</li> <li>Adding s3 configuration also for export and query functions</li> <li>Add locus breaker</li> <li>Remove unwanted files</li> <li>Use Ruff as linter and formatter</li> <li>Memory_budget_mb has been deprecated</li> <li>Remove unwanteed accessory file</li> <li>Add hapmap3 list of SNPs</li> <li>Put main.py back in its place</li> <li>Remove cache files</li> <li>Linting</li> <li>Minor linting fixes</li> <li>Simplify s3 configuration</li> <li>Stops @bruno-ariano from continuing to move the main around</li> <li>Checkout the repository</li> <li>Remove python 3.12</li> <li>Use miniconda env</li> <li>Typo</li> <li>Simplify environment.yml</li> <li>Update python build system</li> <li>Downgrade</li> <li>Disable base</li> <li>Not ignore bash profile files</li> <li>Add pybedtools dependency</li> <li>Downgrade numpy as required by tiledbvcf</li> <li>Add dask dependency</li> <li>Update actions/checkout version</li> <li>Add mongodb models and some test units</li> <li>Update github action</li> <li>Update actions version</li> <li>Still fix for ImportError: cannot import name '_manylinux' from 'packaging'</li> <li>Add is_connected unit test</li> <li>Add hash utils</li> <li>Add compute_sha256 test</li> <li>Remove poetry.lock</li> <li>Update conda-locks</li> <li>Update dependencies</li> <li>Update README</li> <li>Use unique key to retrieve objects</li> <li>Add metadata ingest stub</li> <li>Add click command</li> <li>Save EnhancedDataProfile object</li> <li>Move config inside the app, fix connection manager and add a unit test, optimize imports</li> <li>Improve conda environment</li> <li>Bump version</li> <li>Add view command</li> <li>Add command into main</li> <li>Add meta-view command</li> <li>Add meta-query command</li> <li>Get total_samples from the dataset</li> <li>Put the correct attribute</li> <li>Add query test</li> <li>Update metadata cli</li> <li>Branch for ingestin of tiledb_unified</li> <li>Adjusting for sha</li> <li>Complete ingestion edits</li> <li>Modify export</li> <li>Adding export</li> <li>Adding polars to snp selection</li> <li>Include polars dependency</li> <li>Add pandas dependency</li> <li>Slight changes in the ingestions</li> <li>Slightly adjust parameters</li> <li>Update dependencies; remove pybedtools, add boto</li> <li>Remove EA, NEA attributes</li> <li>Update ingest with different path for s3 and files</li> <li>Remove functions as they are under utils</li> <li>Restore tests</li> <li>Add pyarrow dependency</li> <li>Remove EA, NEA attributes from schema also</li> <li>Catch NotUniqueError</li> <li>Update project name</li> <li>Update meta-ingest</li> <li>Provide an option to use as input the checksum list</li> <li>Do not exit for the moment</li> <li>Relax the string length constraint on the trait_desc argument</li> <li>Use Singleton pattern for the configuration manager</li> <li>Add an example of search template</li> <li>Use src layouts and improve Docker support</li> <li>Update conda environment</li> <li>Implement meta_query traversing dictionary keys</li> </ul>"},{"location":"changelog/#testing_3","title":"\ud83e\uddea Testing","text":"<ul> <li>Test ruff</li> <li>Test ruff 2</li> </ul>"},{"location":"changelog/#020-2023-10-15","title":"[0.2.0] - 2023-10-15","text":""},{"location":"changelog/#bug-fixes_21","title":"\ud83d\udc1b Bug Fixes","text":"<ul> <li>Fix the exporter</li> </ul>"},{"location":"changelog/#other_30","title":"\ud83d\udcbc Other","text":"<ul> <li>First commit</li> <li>Add github workflow</li> <li>Update README</li> <li>Remove pip install poetry</li> <li>Change image</li> <li>Update shell</li> <li>Remove mongomock</li> <li>Change to debian image</li> <li>Set locale</li> <li>Add info and query commands</li> </ul>"},{"location":"commands/","title":"Commands usage","text":""},{"location":"commands/#commands","title":"Commands","text":""},{"location":"commands/#export","title":"<code>export</code>","text":"<p>Export summary statistics from TileDB datasets with various filtering options.</p> <p>Usage:</p> <pre><code>gwasstudio export [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--uri TEXT</code>: URI of the TileDB dataset.</li> <li><code>--output-prefix TEXT</code>: Prefix for naming output files (default: <code>out</code>).</li> <li><code>--output-format [parquet|csv.gz|csv]</code>: Output file format (default: <code>csv.gz</code>).</li> <li><code>--search-file TEXT</code>: Input file for querying metadata (required).</li> <li><code>--attr TEXT</code>: String delimited by comma with the attributes to export (default: <code>BETA,SE,EAF,MLOG10P</code>).</li> </ul> <p>Locusbreaker Options:</p> <ul> <li><code>--locusbreaker</code>: Option to run locusbreaker (flag).</li> <li><code>--pvalue-sig FLOAT</code>: Maximum log p-value threshold within the window (default: <code>5.0</code>).</li> <li><code>--pvalue-limit FLOAT</code>: Log p-value threshold for loci borders (default: <code>3.3</code>).</li> <li><code>--hole-size INTEGER</code>: Minimum pair-base distance between SNPs in different loci (default: <code>250000</code>).</li> <li><code>--maf FLOAT</code>: MAF filter to apply before locusbreaker (default: <code>0.01</code>).</li> <li><code>--locus-flanks INTEGER</code>: Flanking regions (in bp) to extend each locus in both directions (default: <code>100000</code>).</li> <li><code>--phenovar</code>: Boolean to compute phenovariance (Work in progress, not fully implemented yet) (flag).</li> </ul> <p>Regions and SNP ID List Filtering Options:</p> <ul> <li><code>--get-regions-snps TEXT</code>: Bed file (or txt file with CHR and POS columns) with regions or SNPs to filter.</li> <li><code>--nest</code>: Estimate effective population size (Work in progress, not fully implemented yet) (flag).</li> </ul> <p>P-value Filtering Options:</p> <ul> <li><code>--pvalue-thr FLOAT</code>: P-value threshold in -log10 format used to filter significant SNPs (default: 0, no filter)</li> </ul> <p>Plotting Options:</p> <ul> <li><code>--plot-out</code>: Boolean to plot results. If enabled, the output will be plotted as a Manhattan plot (flag).</li> <li><code>--color-thr TEXT</code>: Color for the points passing the threshold line in the plot (default: <code>red</code>).</li> <li><code>--s-value INTEGER</code>: Value for the suggestive p-value line in the plot (default: <code>5</code>).</li> </ul> <p>Query Options:</p> <ul> <li><code>--case-sensitive</code>: Enable case sensitive search of data to export</li> <li><code>--exact-match</code>: Enable exact match search of data to export</li> </ul>"},{"location":"commands/#info","title":"<code>info</code>","text":"<p>Show GWASStudio details</p> <p>Usage:</p> <pre><code>gwasstudio info\n</code></pre>"},{"location":"commands/#ingest","title":"<code>ingest</code>","text":"<p>Ingest data in TileDB datasets.</p> <p>Usage:</p> <pre><code>gwasstudio ingest [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--file-path TEXT</code>: Path to the tabular file containing details for the ingestion (required).</li> <li><code>--delimiter TEXT</code>: Character or regex pattern to treat as the delimiter (default: <code>\\t</code>).</li> <li><code>--uri TEXT</code>: Destination path where to store the tiledb dataset. The prefix can be <code>s3://</code> or <code>file://</code> (required).</li> <li><code>--ingestion-type [metadata|data|both]</code>: Choose between metadata ingestion, data ingestion, or both (default: <code>both</code>).</li> <li><code>--pvalue</code>: Indicate whether to ingest the p-value from the summary statistics instead of calculating it (default: <code>True</code>).</li> </ul>"},{"location":"commands/#list","title":"<code>list</code>","text":"<p>List every category \u2192 project \u2192 study hierarchy stored in the metadata DB</p> <p>Usage:</p> <pre><code>gwasstudio list\n</code></pre>"},{"location":"commands/#meta-query","title":"<code>meta-query</code>","text":"<p>Query metadata records from MongoDB</p> <p>Usage:</p> <pre><code>gwasstudio meta-query [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--search-file</code>: The search file used for querying metadata  [required]</li> <li><code>--output-prefix</code>: Prefix to be used for naming the output files</li> <li><code>--case-sensitive</code>: Enable case sensitive search</li> <li><code>--exact-match</code>: Enable exact match search</li> </ul>"},{"location":"configuration/","title":"GWASStudio: Configuration","text":"<p>How to configure access to the diverse components used by GWASStudio.</p>"},{"location":"examples/","title":"Examples","text":"<p>The following examples assume you have already set up the GWASStudio CLI.</p>"},{"location":"examples/#retrieving-the-full-list-of-trait-descriptions-from-a-study","title":"Retrieving the full list of trait descriptions from a study","text":"<p>In this example, we retrieve every trait descriptions from a specific project/study. The chosen study is \"believe\", which belongs to the \"pqtl\" project.</p> <p>First, let's create a query file named <code>search_believe.yml</code> containing the following:</p> <pre><code>project: pqtl\nstudy: believe\n\noutput:\n  - trait_desc\n</code></pre> <p>In the YAML file, we have specified the project and study that we want to query, as well as the field that we want in the output.</p> <p>Then, we can run the query using the following command:</p> <pre><code>gwasstudio meta-query --search-file search_believe.yml\n</code></pre> <p>You\u2019ll see a log similar to:</p> <pre><code>2025-10-08 15:59:56.294 | INFO     | Gwasstudio started\n2025-10-08 15:59:56.295 | INFO     | Processing search_believe.yml\n2025-10-08 15:59:57.013 | INFO     | 7244 results found. Writing to out_meta.csv\n</code></pre> <p>The command reads the YAML file, queries the metadata database, and writes the results to <code>out_meta.csv</code> in the current directory. Use <code>--output-prefix &lt;prefix&gt;</code> to change the output file\u2019s prefix.</p>"},{"location":"examples/#filtering-a-study-by-snps-on-some-traits-pqtl-study","title":"Filtering a study by SNPs on some traits - pQTL study","text":"<p>This example shows how to filter a study by a list of SNPs and retrieve the corresponding summary statistics.</p> <p>The chosen study is \"believe\", which belongs to the \"pqtl\" project.</p> <p>First, let's create a query file named <code>search_believe.yml</code> containing the following:</p> <pre><code>project: pqtl\nstudy: believe\n\ntrait:\n  - seqid: 10000-28\n  - seqid: 7059-14\n  - seqid: 9995-6\n\noutput:\n  - build\n  - notes_sex\n  - notes_source_id\n  - trait_desc\n  - total_samples\n</code></pre> <p>In the YAML file, we have specified the project and study that we want to query, as well as some specific seqid traits.</p> <p>The <code>trait</code> block limits the export to the specified <code>seqid</code> traits.</p> <p>Secondly, we need a list of SNPs that we use for filtering the data. We can create a file named <code>snp_list.txt</code> containing the following:</p> <pre><code>CHR,POS\n4,29721067\n5,157004314\n6,167099480\n7,31837152\n7,140838327\n8,131244697\n10,25816112\n11,11521474\n12,70397835\n21,43962980\n</code></pre> <p>Then, we can run the <code>export</code> using the following command:</p> <pre><code>gwasstudio export --search-file search_believe.yml --get-regions-snps snp_list.txt\n</code></pre> <p>Sample log output:</p> <pre><code>2025-10-08 17:05:50.707 | INFO     | Gwasstudio started\n2025-10-08 17:05:50.737 | INFO     | Processing search_believe.yml\n2025-10-08 17:05:51.488 | INFO     | 3 results found. Writing to out_meta.csv\n2025-10-08 17:05:53.877 | INFO     | Dask local cluster: starting 2 workers, with 4GiB of memory and 2 cpus per worker\n2025-10-08 17:05:53.877 | INFO     | Dask cluster dashboard: http://127.0.0.1:8787/status\n2025-10-08 17:05:53.889 | INFO     | Processing the group pqtl_believe\n2025-10-08 17:05:53.890 | INFO     | Running batch 1/1 (4 items)\n2025-10-08 17:06:55.436 | INFO     | Batch 1 completed.\n2025-10-08 17:06:55.437 | INFO     | Shutting down Dask client and cluster.\n</code></pre> <p>The resulting summary\u2011statistics tables are written to the current directory. Again, you can change the output file prefix with <code>--output-prefix</code> ."},{"location":"examples/#filtering-a-study-by-snps-on-some-traits-gwas-study","title":"Filtering a study by SNPs on some traits - GWAS study","text":"<p>This example shows how to filter a study by a list of SNPs and retrieve the corresponding summary statistics.</p> <p>The chosen study is \"ukb-d\", which belongs to the \"opengwas\" project.</p> <p>First, let's create a query file named <code>search_ukb.yml</code> containing the following:</p> <pre><code>project: opengwas\nstudy: ukb-d\n\ntrait:\n  - desc: heart failure\n\noutput:\n  - build\n  - notes_sex\n  - notes_source_id\n  - total_samples\n  - total_cases\n  - total_controls\n  - trait_desc\n</code></pre> <p>By default, GWASStudio performs a case-insensitive substring search of the trait description. This query matches four traits that contain 'heart failure' anywhere in the description.</p> <p>Secondly, we need a list of SNPs that we use for filtering the data. We can create a file named <code>snp_list.txt</code> containing the following:</p> <pre><code>CHR,POS\n4,29721067\n5,157004314\n6,167099480\n7,31837152\n7,140838327\n8,131244697\n10,25816112\n11,11521474\n12,70397835\n21,43962980\n</code></pre> <p>Then, we can run the <code>export</code> using the following command:</p> <pre><code>gwasstudio export --search-file search_ukb.yml --get-regions-snps snp_list.txt\n</code></pre> <p>Log snippet:</p> <pre><code>2025-10-08 18:35:36.884 | INFO     | Gwasstudio started\n2025-10-08 18:35:36.925 | INFO     | Processing search_ukb.yml\n2025-10-08 18:35:37.378 | INFO     | 4 results found. Writing to out_meta.csv\n2025-10-08 18:35:40.466 | INFO     | Dask local cluster: starting 2 workers, with 4GiB of memory and 2 cpus per worker\n2025-10-08 18:35:40.467 | INFO     | Dask cluster dashboard: http://127.0.0.1:8787/status\n2025-10-08 18:35:40.532 | INFO     | Processing the group opengwas_ukb-d\n2025-10-08 18:35:40.534 | INFO     | Running batch 1/1 (4 items)\n2025-10-08 18:36:04.388 | INFO     | Batch 1 completed.\n2025-10-08 18:36:04.389 | INFO     | Shutting down Dask client and cluster.\n</code></pre> <p>Results are written to the current directory; use <code>--output\u2011prefix</code> to customise the filename.</p>"},{"location":"getting-started/","title":"Getting Started","text":"<p>This short overview of GWASStudio will help you get started.</p>"},{"location":"getting-started/#usage-on-the-ht-hpc","title":"Usage on the HT-HPC","text":"<p>To use GWASStudio on the HT computing cluster, simply type:</p> <pre><code>source /exchange/healthds/singularity_functions\n</code></pre> <p>Verify with:</p> <pre><code>gwasstudio --version\n</code></pre>"},{"location":"getting-started/#vault-token","title":"Vault token","text":"<p>To securely access (meta)data according to your user permissions, you will be provided with a vault token.</p> <p>To authenticate automatically, please save your token in the following file:</p> <pre><code>${HOME}/.vault-token\n</code></pre> <p>NOTE:</p> <ul> <li>The vault token is personal and confidential. Do not share it with other users</li> <li>If <code>${HOME}/.vault-token</code> is missing, you will be prompted to manually paste the token during commmand execution</li> </ul>"},{"location":"getting-started/#main-commands","title":"Main commands","text":"<p>GWASStudio has three main commands for users:</p> <ul> <li>list: list available/accessible data</li> <li>meta-query: query metadata of interest</li> <li>ingest: ingestion of summary-statistics files(s)</li> <li>export: export data of interest</li> </ul>"},{"location":"getting-started/#1-list","title":"1. <code>list</code>","text":"<p>The <code>list</code> command is used to display all summary statistics available on MongoDB, based on your access permissions (see vault token) as category \u2192 project \u2192 study.</p>"},{"location":"getting-started/#list-example","title":"List example","text":"<pre><code>gwasstudio list\n</code></pre>"},{"location":"getting-started/#list-output-example","title":"List output example","text":"<pre><code>Category: GWAS\n  Project: opengwas\n        Studies: ukb-a, ukb-b, ukb-d\n</code></pre>"},{"location":"getting-started/#2-meta-query","title":"2. <code>meta-query</code>","text":"<p>The <code>meta-query</code> command retrieves metadata of interest using a query file. It can be used to verify the availability and characteristics of the data to export.</p>"},{"location":"getting-started/#meta-query-example","title":"Meta-query example","text":"<pre><code>gwasstudio meta-query --search-file query_ex01.txt --output-prefix output_query_ex01\n</code></pre> <p>The output is a metadata table named output_query_ex01.csv with records filtered by the query file query_ex01.txt.</p> <p>For a detailed explanation of all command options, see also meta-query command.</p>"},{"location":"getting-started/#query-file","title":"Query file","text":"<p>The query file used to retrieve (meta)data follows a structured format with two sections:</p> <ul> <li>Filtering criteria: metadata fields used to query the database, specified as <code>metadata field: filtering value</code> pairs</li> <li>Output specification (<code>output:</code>): a list of valid metadata fields to include in the output</li> </ul>"},{"location":"getting-started/#query-file-example","title":"Query file example","text":"<pre><code>project: opengwas\nstudy: ukb-d\n\ntrait:\n  - desc: Z42\n  - desc: pregnancy\n\noutput:\n  - build\n  - population\n  - notes_sex\n  - notes_source_id\n  - total_samples\n  - total_cases\n  - total_controls\n  - trait_desc\n</code></pre> <p>This query file searches within the <code>ukb-d</code> study for all trait descriptions containing <code>Z42</code> or <code>pregnancy</code>, and returns a table with the columns specified in section <code>output:</code>.</p> <p>NOTES:</p> <ul> <li>Filtering values can include partial matches  (e.g. trait descriptions containing <code>Z42</code> or <code>pregnancy</code>)</li> <li>Filtering values are processed by lowercasing and replacing special characters before being used to query the database</li> <li>It is possible to query across different projects and studies by not specifiyng <code>project</code> and <code>study</code> in the query file</li> </ul>"},{"location":"getting-started/#meta-query-output-example","title":"Meta-query output example","text":"project study category data_id build population notes_sex notes_source_id total_samples total_cases total_controls trait_desc opengwas ukb-d GWAS 47e96deafe GRCh38 European Males and Females ukb-d-XV_PREGNANCY_BIRTH 361194 11959 349235 \"Pregnancy,  childbirth and the puerperium\" opengwas ukb-d GWAS 531f0d4bcc GRCh38 European Males and Females ukb-d-Z42 361194 1963 359231 Diagnoses - main ICD10: Z42 Follow-up care involving plastic surgery opengwas ukb-d GWAS cc18ce8683 GRCh38 European Males and Females ukb-d-O26 361194 1289 359905 Diagnoses - main ICD10: O26 Maternal care for other conditions predominantly related to pregnancy"},{"location":"getting-started/#3-ingest","title":"3. <code>ingest</code>","text":"<p>The <code>ingest</code> command stores harmonized summary-statistics files(s) into a TileDB dataset, , using the relative metadata (which includes the source file paths) and the specified destination path.</p> <p>For a detailed explanation of input formatting, see Summary-statistics columns and Metadata fields.</p>"},{"location":"getting-started/#ingest-example","title":"Ingest example","text":"<pre><code>gwasstudio ingest --file-path metadata_ukb_d_sampled.tsv --uri destination\n</code></pre> <p>This command creates a folder named <code>destination</code>, containing the summary-statistics data stored in TileDB format.</p> <p>For a detailed explanation of all command options, see also ingest command.</p>"},{"location":"getting-started/#4-export","title":"4. <code>export</code>","text":"<p>The <code>export</code> command is used to extract records of summary statistics (and associated metadata) from TileDB as speficied in the query file.</p>"},{"location":"getting-started/#enter-compute-node","title":"Enter compute node","text":"<p>The <code>export</code> command is a computationally intensive operation. Therefore, it must be executed from a compute node.</p> <p>To enter a compute node, run the following command:</p> <pre><code>salloc --partition=cpu-interactive --nodes=1 --ntasks-per-node=2 --mem-per-cpu=2048M --time=12:00:00\n</code></pre>"},{"location":"getting-started/#full-stats","title":"Full stats","text":"<p>The <code>export</code> command, when used without any filtering options, will export the full set of summary statistics.</p> <pre><code>gwasstudio export --search-file query_ex01.txt\n</code></pre>"},{"location":"getting-started/#filtering-options","title":"Filtering options","text":"<p>Exports can also be performed with different filtering options.</p>"},{"location":"getting-started/#region-and-snp-filtering","title":"Region and SNP filtering","text":"<p>Command example to export data by filtering regions and SNPIDs provided in <code>region_or_snp_list.tsv</code>:</p> <pre><code>gwasstudio export --search-file query_ex01.txt --get-regions-snps region_or_snp_list.tsv\n</code></pre> <p>The list of regions and SNPs to filter should preferably be in BED format. Example: <code>regions_query.tsv</code>.</p> <p>Alternatively, SNPIDs can also be listed in CHR,POS format. Example: <code>hapmap3_snps.csv</code>.</p>"},{"location":"getting-started/#p-value-filtering","title":"P-value filtering","text":"<p>Command example to export data by filtering based on a P-value threshold (in -log10 format):</p> <pre><code>gwasstudio export --search-file query_ex01.txt --pvalue-thr 4\n</code></pre>"},{"location":"getting-started/#locusbreaker","title":"Locusbreaker","text":"<p>Command example to export data with locusbreaker:</p> <pre><code>gwasstudio export --search-file query_ex01.txt --locusbreaker\n</code></pre> <p>For a detailed explanation of all command options, see also export command.</p>"},{"location":"installation/","title":"GWASStudio: Installation Guide","text":"<p>This guide will walk you through the installation process.</p>"},{"location":"installation/#installation-from-source","title":"Installation from source","text":"<p>You can install GWASStudio using either the <code>make</code> command (recommended for simplicity) or manually. Choose the method that best fits your workflow.</p>"},{"location":"installation/#1-installation-using-make-recommended","title":"1. Installation Using <code>make</code> (Recommended)","text":"<p>The <code>make</code> method automates most of the installation steps, making it easier and less error-prone.</p>"},{"location":"installation/#prerequisites","title":"Prerequisites","text":"<ul> <li>Ensure you have <code>make</code> installed on your system.</li> <li>Ensure you have <code>conda</code> or <code>miniconda</code> installed for environment management.</li> </ul>"},{"location":"installation/#steps","title":"Steps","text":"<p>1. Clone the Repository</p> <p>Clone the repository to your local machine:</p> <pre><code>git clone https://github.com/ht-diva/gwasstudio.git\ncd gwasstudio\n</code></pre> <p>2. Create the Conda Environment</p> <p>Run the following command to create a dedicated conda environment for GWASStudio:</p> <pre><code>make create-env\n</code></pre> <p>This command reads the environment configuration from <code>base_environment.yml</code> and sets up a clean, isolated environment.</p> <p>3. Install Dependencies</p> <p>Install all required dependencies using:</p> <pre><code>make dependencies\n</code></pre> <p>This step ensures that all necessary Python packages and system libraries are installed.</p> <p>4. Install the Program</p> <p>Finally, install the program in your conda environment:</p> <pre><code>make install\n</code></pre> <p>This installs the program globally within your conda environment, making it available from the command line.</p>"},{"location":"installation/#2-manual-installation-without-make","title":"2. Manual Installation (Without <code>make</code>)","text":"<p>If you prefer not to use <code>make</code>, you can install GWASStudio manually. This method gives you more control over each step.</p>"},{"location":"installation/#prerequisites_1","title":"Prerequisites","text":"<ul> <li>Ensure you have <code>conda</code> or <code>miniconda</code> installed.</li> </ul>"},{"location":"installation/#steps_1","title":"Steps","text":"<p>1. Clone the Repository</p> <p>Clone the repository to your local machine:</p> <pre><code>git clone https://github.com/ht-diva/gwasstudio.git\ncd gwasstudio\n</code></pre> <p>1. Create the Conda Environment</p> <p>Create a new conda environment using the provided configuration file:</p> <pre><code>conda env create --file base_environment.yml\n</code></pre> <p>This command sets up a conda environment with all the base dependencies specified in <code>base_environment.yml</code>.</p> <p>2. Activate the Conda Environment</p> <p>Activate the environment with the following commands:</p> <pre><code>  conda activate gwasstudio\n</code></pre> <p>4. Install the program</p> <p>Use <code>poetry</code> to install the project dependencies (excluding development dependencies) and the program:</p> <pre><code>  poetry install --without dev\n</code></pre> <p>This installs gwasstudio in your active conda environment.</p>"},{"location":"metadata/","title":"Metadata records","text":"<p>Each project-study combination is associated with a metadata record that describes the key characteristics of the available summary-statistics file(s). These records are used throughout the platform (e.g., for filtering, querying, and API responses) and therefore follow a stable schema. When new attributes become relevant they should be added to this table and, if possible, given a concise, human-readable label.</p>"},{"location":"metadata/#core-metadata-fields","title":"Core metadata fields","text":"Field (canonical name) Friendly label Description Possible values / format <code>category</code> Summary-statistic type Type of summary statistics <code>GWAS</code>, <code>pQTL</code>, <code>eQTL</code>, \u2026 <code>project</code> Project Identifier of the project the data belongs to <code>opengwas</code>, <code>pqtl</code>, <code>genesandhealth</code>, \u2026 <code>study</code> Study Identifier of the specific study <code>ukb-a</code>, <code>ukb-b</code>, <code>ukb-d</code>, \u2026 <code>data_id</code> Record ID Unique identifier for the metadata entry e.g. <code>89f31189b3</code> <code>file_path</code> File paths The paths where the summary-statistics files are stored e.g. <code>./ukb-d_sampled/ukb-d-XVIII_MISCFINDINGS.gwaslab.tsv.sampled.gz</code> <code>build</code> Genome build Reference genome build <code>GRCh37</code>, <code>GRCh38</code> <code>population</code> Broad ancestry category Ancestry of the cohort see the ancestry categories page <code>total_samples</code> Number of samples Sample size of the cohort integer <code>total_cases</code> Number of cases Count of case subjects (if applicable) integer <code>total_controls</code> Number of controls Count of control subjects (if applicable) integer <code>trait_desc</code> Trait description Human-readable description of the phenotype or protein e.g. \u201cPregnancy, childbirth and the puerperium\u201d, \u201cAlpha-1B-glycoprotein\u201d <code>notes_sex</code> Sex of participants Sex composition of the cohort <code>Males</code>, <code>Females</code>, <code>Combined</code>"},{"location":"metadata/#additional-optional-fields-may-appear-in-specific-projects","title":"Additional optional fields (may appear in specific projects)","text":"Field Friendly label When to use <code>notes_maker</code> Data producer Institution or consortium that generated the data <code>notes_maker_platform_technology</code> Assay technology Technology used to generate the data (e.g., array, sequencing) <code>notes_maker_platform_description</code> Assay description Provide extra detail beyond the technology name <code>notes_maker_platform_version</code> Assay version Useful when multiple releases exist <code>notes_maker_platform_normalization</code> Normalization strategy Clarifies how raw data were transformed <code>notes_software_description</code> Analysis software Record the tool that generated the statistics <code>notes_source_id</code> Source ID Original identifier of the upstream summary\u2011statistics file <code>trait_gene_ids</code> Gene identifiers When multiple genes are linked to the trait <code>trait_icd10</code> Clinical code When the trait is a disease phenotype <code>trait_protein_ids</code> Protein identifiers For proteomics\u2011related summary statistics <code>trait_seqid</code> Sequence identifier SomaLogic specific sequence identifier for the trait <code>trait_tissue</code> Measured tissue eQTL/pQTL studies that are tissue\u2011specific <code>trait_unit</code> Measurement unit For quantitative traits (e.g., blood pressure)"},{"location":"population/","title":"Ancestry categories","text":"<p>The GWAS\u2011Studio platform uses a standardised list of ancestry (population) categories. Each category has a short, machine\u2011friendly code (the shortcut) that can be stored in the database or used in API calls, while the full description is shown to users.</p>"},{"location":"population/#ancestry-shortcut-table","title":"Ancestry shortcut table","text":"Shortcut Full description AUS Aboriginal Australian AFA African American or Afro\u2011Caribbean AFR African unspecified ASN Asian unspecified CAS Central Asian EAS East Asian EUR European MDE Greater Middle Eastern (Middle Eastern, North African, or Persian) AMR Hispanic or Latin American ISL Icelandic NR Not reported / unknown NAM Native American OCE Oceanian OTH Other ADM Other admixed ancestry SAS South Asian SEA South East Asian SAF Sub\u2011Saharan African"},{"location":"population/#how-to-use-the-table","title":"How to use the table","text":"<ul> <li>Database / API \u2013 store the shortcut (e.g., <code>EUR</code>) and map it to the full description when presenting data to users.</li> <li>User interfaces \u2013 show the full description in dropdowns or tooltips; the shortcut can be used for filtering or as a compact label.</li> <li>Validation \u2013 accept either the shortcut or the full description, then normalise to the shortcut for internal consistency.</li> </ul> <p>The list is version\u2011controlled; any addition or change should be reflected both in <code>src/gwasstudio/config/config.yaml</code> (under <code>ancestry_groups</code>) and in this documentation.</p> <p>Source: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5815218/table/Tab1/?report=objectonly</p>"},{"location":"projects/","title":"Projects","text":"<p>This is an overview of the GWASStudio projects available on-premises at Human Tecnopole, along with the relevant core metadata fields (see metadata.md#core-metadata-fields).</p> <p>Note: All summary statistics were harmonized to the GWASLab format via harmonization_pipeline.</p> Project name <code>category</code> <code>project</code> <code>study</code> deCODE <code>pQTL</code> <code>DECODE</code> <code>large scale plasma 2023</code> FinnGen <code>GWAS</code> <code>finngen</code> <code>R12</code> Genes &amp; Health <code>GWAS</code> <code>genesandhealth</code> <code>v010_binary_traits_3digitICD10</code>, <code>v010_quantitative_traits_median_values</code> UKB <code>GWAS</code> <code>opengwas</code> <code>ukb-a</code>, <code>ukb-b</code>, <code>ukb-d</code> UKB-PPP <code>pQTL</code> <code>ukb-ppp_consortium</code> <code>ukb-ppp_African</code>, <code>ukb-ppp_American</code>, <code>ukb-ppp_Combined</code>, <code>ukb-ppp_East_Asian</code>, <code>ukb-ppp_European</code>, <code>ukb-ppp_Middle_Eastern</code>, <code>ukb-ppp_South_Asian</code>"},{"location":"projects/#decode","title":"deCODE","text":"<p>The deCODE project is a large-scale population genetics initiative that integrates medical and genetic data from over 160,000 Icelandic participants. This includes genotype information, whole-genome sequencing, quantitative traits, binary disease phenotypes, and extensive genealogical records. Through global collaborations with medical institutions, deCODE has also incorporated data from ~500,000 individuals worldwide. A primary goal is to identify genetic risk factors for common diseases (e.g., heart attack, asthma, stroke, and cancer) and to advance personalized medicine.</p> <p>GWASStudio includes 5,284 summary statistics from the large-scale plasma proteomics GWAS conducted by deCODE.</p> <p>For more information on the deCODE Project, see the deCODE genetics Inc. article.</p> <p>For details on the plasma proteomics GWAS, see the original article and the 2023 follow-up study that integrates results with data from UKB-PPP: UKB-deCODE comparative analysis.</p>"},{"location":"projects/#key-metadata","title":"Key Metadata","text":"<ul> <li>Number of Protein Analytes<ul> <li>4,907 aptamers (measuring 4,719 proteins)</li> </ul> </li> <li>Protein Platform<ul> <li>SomaScan multiplex aptamer assay (version 4)</li> </ul> </li> <li>Number of Variants<ul> <li>~27.2 million imputed variants</li> </ul> </li> <li>Imputation Panel<ul> <li>Whole-genome sequencing (WGS on ~60,000 Icelanders) used as population-specific reference panel for genotype imputation into the larger Icelandic cohort (~160,000 individuals)</li> </ul> </li> <li>Genome build<ul> <li>GRCh38</li> </ul> </li> <li>Sample Size and Ancestry<ul> <li>~36,000 Icelandic individuals (i.e. primarily Icelandic and European ancestry)</li> </ul> </li> <li>GWAS Method<ul> <li>linear mixed model (BOLT-LMM)</li> </ul> </li> </ul> <p>Note: For detailed GWAS methodology, see the original article.</p>"},{"location":"projects/#finngen","title":"FinnGen","text":"<p>FinnGen is a large-scale research initiative that integrates genomic data with national health registry data from 500,000 Finnish biobank participants. The project aims to identify genetic risk factors for disease, understand disease progression and treatment response, and advance personalized medicine.</p> <p>GWASStudio includes 2,469 summary statistics from the FinnGen GWAS R12 analysis.</p> <p>For more information on the FinnGen Project and methods, see the FinnGen documentation and FinnGen Handbook.</p> <p>To browse FinnGen traits (including trait definition, prevalence, and longitudinal comorbidities), see the Risteys portal.</p>"},{"location":"projects/#key-metadata_1","title":"Key Metadata","text":"<ul> <li>Number of Traits<ul> <li>2,469</li> </ul> </li> <li>Trait Types<ul> <li>2,466 binary traits</li> <li>3 quantitative traits (<code>HEIGHT_IRN</code>, <code>WEIGHT_IRN</code>, <code>BMI_IRN</code>)</li> </ul> </li> <li>Number of Variants<ul> <li>~21 million imputed variants</li> </ul> </li> <li>Imputation Panel<ul> <li>Sisu v4.2 reference panel (Finnish-specific reference panel combining high-coverage WGS and WES)</li> </ul> </li> <li>Genome build<ul> <li>GRCh38</li> </ul> </li> <li>Sample Size and Ancestry<ul> <li>500,348 individuals of primarily Finnish/European ancestry</li> </ul> </li> <li>GWAS Method<ul> <li>regenie v2.2.4 (v3.3 used for traits not converging under v2.2.4)</li> </ul> </li> </ul> <p>Note: For detailed GWAS methodology, see the FinnGen documentation.</p>"},{"location":"projects/#genes-health","title":"Genes &amp; Health","text":"<p>Genes &amp; Health is a community\u2011based genetic research initiative, focusing on British Bangladeshi and British Pakistani communities, with over 100,000 participants. The project aims to reduce health disparities in these groups (e.g. in diabetes, cardiovascular diseases, and mental health disorders) by researching genetic risk factors, disease progression, treatment response, and supporting more inclusive precision medicine.</p> <p>GWASStudio includes 762 binary\u2011trait summary statistics and 107 quantitative\u2011trait summary statistics from Genes &amp; Health (2025 Realease).</p> <p>For more information on the Genes &amp; Health project, data access, and methods, see the Genes &amp; Health \"Data &amp; Researchers\" page and the project website.</p>"},{"location":"projects/#key-metadata_2","title":"Key Metadata","text":"<ul> <li>Number of Traits<ul> <li><code>v010_binary_traits_3digitICD10</code>: 762 binary traits</li> <li><code>v010_quantitative_traits_median_values</code>: 107 quantitative traits (median values)</li> </ul> </li> <li>Trait Types<ul> <li><code>v010_binary_traits_3digitICD10</code>: ICD10-coded binary traits (e.g. A01\u2013Q99)</li> <li><code>v010_quantitative_traits_median_values</code>: quantitative (e.g. BMI, LDL-cholesterol)</li> </ul> </li> <li>Number of Variants<ul> <li>~37.5 million imputed variants</li> </ul> </li> <li>Imputation Panel<ul> <li>TOPMed-r3</li> </ul> </li> <li>Genome build<ul> <li>GRCh38</li> </ul> </li> <li>Sample Size and Ancestry<ul> <li>~100,000 participants of British Bangladeshi and British Pakistani (South Asian) ancestry</li> </ul> </li> <li>GWAS Method<ul> <li>regenie</li> </ul> </li> </ul> <p>Note: For quantitative traits, Genes &amp; Health provides 321 quantitative\u2011trait summary statistics derived from minimum, median, and maximum observed values per individual. For consistency and to reduce variability caused by extreme measurements or outliers, median values were selected to represent each quantitative trait in the GWAS analyses included in GWASStudio.</p> <p>Note: For more information on the phenotypes and notation, see the dedicated page on the project website.</p>"},{"location":"projects/#uk-biobank-ukb","title":"UK Biobank (UKB)","text":"<p>UK Biobank (UKB) is a large-scale biomedical database and research resource containing detailed health, lifestyle, environmental, and genetic data from approximately 500,000 UK participants aged 40\u201369 at recruitment (2006\u20132010). Participants have been followed longitudinally through linked electronic health records, imaging, biomarker assays, and repeat assessments.</p> <p>A wide range of GWAS analyses have been conducted on UKB phenotypes by multiple research groups, often focusing on different trait types, QC pipelines, or analysis methods. GWASStudio includes results from three major sources, for a total of 3,945 UKB-based GWAS summary statistics:</p> <ul> <li><code>ukb-a</code>: Neale Lab Phase 1 Release (2017) \u2014 early GWAS on core phenotype (596)</li> <li><code>ukb-b</code>: MRC-IEU OpenGWAS project (2021+) \u2014 harmonized and quality-controlled GWAS across thousands of traits (2,514)</li> <li><code>ukb-d</code>: Neale Lab Phase 2/3 Release (2018) \u2014 expanded GWAS with improved imputation, additional phenotypes, and updated QC pipelines (835)</li> </ul> <p>For more information on the UK Biobank project, see https://www.ukbiobank.ac.uk/ and UKB Data Showcase, which provides searchable metadata on all available phenotypes and samples.</p>"},{"location":"projects/#key-metadata_3","title":"Key Metadata","text":"<ul> <li>Number of Traits<ul> <li><code>ukb-a</code>: 596</li> <li><code>ukb-b</code>: 2,514</li> <li><code>ukb-d</code>: 835</li> </ul> </li> <li>Trait Types<ul> <li><code>ukb-a</code> and <code>ukb-d</code>: binary (disease case-control), continuous (physical and lab measures)</li> <li><code>ukb-b</code>: binary, continuous, categorical ordered (e.g. job satisfaction)</li> </ul> </li> <li>Number of Variants<ul> <li>All studies begin with ~17 million imputed variants</li> <li><code>ukb-a</code>: filtered to ~10.5 million</li> <li><code>ukb-b</code>: filtered to ~10 million</li> <li><code>ukb-d</code>: filtered to ~13.5 million</li> </ul> </li> <li>Imputation Panel<ul> <li>UK Biobank Imputation Release v3 (R3)</li> </ul> </li> <li>Genome build<ul> <li>GRCh38 (lifted via GWASLab liftover method)</li> </ul> </li> <li>Sample Size and Ancestry<ul> <li><code>ukb-a</code>: ~337,000 unrelated White British individuals</li> <li><code>ukb-b</code>: ~460,000 full UKB cohort (mostly European), including related individuals</li> <li><code>ukb-d</code>: ~361,000 White British individuals, including related individuals</li> </ul> </li> <li>GWAS Method<ul> <li><code>ukb-a</code> and <code>ukb-d</code>: linear regression (Hail, <code>linreg3</code>)</li> <li><code>ukb-b</code>: linear mixed model (BOLT-LMM or SAIGE, depending on the trait)</li> </ul> </li> </ul> <p>Note: For detailed GWAS methodology, see:</p> <ul> <li><code>ukb-a</code> and <code>ukb-d</code>: GitHub repo and Blog</li> <li><code>ukb-b</code>: GitHub repo and Documentation</li> </ul>"},{"location":"projects/#uk-biobank-pharma-proteomics-project-ukb-ppp","title":"UK Biobank Pharma Proteomics Project (UKB-PPP)","text":"<p>UK Biobank Pharma Proteomics Project (UKB-PPP) is a biopharmaceutical consortium that performed plasma proteomic profiling of 54,219 UK Biobank participants. Protein quantitative trait loci (pQTL) analyses were conducted across European individuals and five additional non-European ancestry groups.</p> <p>GWASStudio includes a total of 20,576 UKB-PPP GWAS summary statistics.</p> <p>For more information on the UKB-PPP project, see the original UKB-PPP article.</p>"},{"location":"projects/#key-metadata_4","title":"Key Metadata","text":"<ul> <li>Number of Protein Analytes:<ul> <li>2,941 protein analytes (2,943 unique proteins)</li> </ul> </li> <li>Protein Panel:<ul> <li>Olink Explore 3072 Panel</li> </ul> </li> <li>Number of Variants:<ul> <li>~23.8 million imputed variants</li> </ul> </li> <li>Genome build:<ul> <li>GRCh38</li> </ul> </li> <li>Sample Size and Ancestry:<ul> <li><code>ukb-ppp_European</code>: 34,557 European individuals</li> <li><code>ukb-ppp_African</code>: 931 African individuals</li> <li><code>ukb-ppp_South_Asian</code>: 920 Central South Asian individuals</li> <li><code>ukb-ppp_Middle_Eastern</code>: 308 Middle Eastern individuals</li> <li><code>ukb-ppp_East_Asian</code>: 262 East Asian individuals</li> <li><code>ukb-ppp_American</code>: 97 Admixed American (mostly Hispanic/Latino groups)</li> <li><code>ukb-ppp_Combined</code>: 52,363 individuals in total \u2014 the combined UKB-PPP cohort includes all above groups plus 17,806 additional European individuals from the replication cohort. The combined cohort results from QC filtering starting from 54,219 initial participants selected by the UKB-PPP consortium</li> </ul> </li> <li>GWAS Method<ul> <li>regenie v2.2.1</li> </ul> </li> </ul> <p>Note: Ancestry groups in UKB-PPP are defined following the pan-UKBB framework, based on clustering individuals in principal component (PC) space using reference populations from the 1000 Genomes Project and the Human Genome Diversity Panel. For more information, see the pan-UKBB study design.</p> <p>Note: For detailed GWAS methodology, see the original UKB-PPP article.</p>"},{"location":"summary-statistics/","title":"Summary Statistics","text":"<p>GWASStudio requires summary-statistics files(s) to be harmonized and formatted in tsv.gz.</p>"},{"location":"summary-statistics/#core-summary-statistics-columns","title":"Core Summary Statistics columns","text":"Column Description <code>CHR</code> Chromomsome in integer format <code>POS</code> Position of the variant <code>EA</code> Effect allele <code>NEA</code> Non-effect alle <code>EAF</code> Effect allele frequency <code>BETA</code> Effect size <code>SE</code> Standard error"},{"location":"summary-statistics/#additional-optional-columns","title":"Additional optional columns","text":"Column Description <code>Z</code> Z-score <code>MLOG10P</code> p-value in -log10 format <code>rsID</code> SNP ID following dbSNP convention <code>SNPID</code> SNPID described as CHR:POS:A1:A2 with A1 and A2 being the alleles in alphabetical order <code>N</code> Sample size <code>STATUS</code> Sample size"},{"location":"summary-statistics/#note","title":"Note","text":"<p>To harmonize and format data in the appropriate way, please refer to the harmonization_pipeline.</p>"}]}